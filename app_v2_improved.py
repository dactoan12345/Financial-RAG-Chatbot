# app_v2_improved.py
import os
import re 
import streamlit as st
from dotenv import load_dotenv
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from qa_system import FinancialQASystem

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="Pro Financial Q&A", page_icon="üìà", layout="wide")

# --- C√ÅC THAM S·ªê V√Ä BI·∫æN TO√ÄN C·ª§C ---
MODEL_NAME = 'all-MiniLM-L6-v2'
PINECONE_INDEX_NAME = 'financial-qa' 

# --- DANH S√ÅCH TICKERS ---
AVAILABLE_TICKERS = sorted([
    "NVDA", "LULU", "BRK-A", "AEN", "ICE", "AAPL", "PG", "META", "VSC", "INTU", 
    "TSLA", "COST", "AXP", "PHE", "IRM", "ABNB", "PTON", "DAL", "JNJ", "JPM", 
    "MSFT", "SBUX", "TRDL", "KR", "LVS", "AMZN", "NKE", "EBAY", "HD", "WMT", 
    "NFLX", "PLTR", "AMD", "CVX", "GOOGL", "ABBV", "BAC", "KO", "V", "GME", 
    "EFX", "T", "AZO", "AMC", "CRM", "ETSY", "CAT", "SCHW", "LLY", "AVGO", 
    "FDX", "CMG", "CB", "UNH", "F", "GRMN", "GIS", "GM", "GILD", "GS", "HAS", 
    "HSY", "HPE", "LTH", "HPQ", "HUM", "IBM"
])

# --- T·∫¢I API KEYS ---
load_dotenv()
gemini_api_key = os.getenv("GEMINI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")

# --- KH·ªûI T·∫†O D·ªäCH V·ª§ (CACHE L·∫†I) ---
@st.cache_resource
def initialize_services():
    if not gemini_api_key or not pinecone_api_key:
        st.error("Vui l√≤ng cung c·∫•p GEMINI_API_KEY v√† PINECONE_API_KEY trong file .env")
        st.stop()
        
    genai.configure(api_key=gemini_api_key)
    gemini_model = genai.GenerativeModel('gemini-2.5-flash')
    
    pc = Pinecone(api_key=pinecone_api_key)

    if PINECONE_INDEX_NAME not in [index.name for index in pc.list_indexes()]:
        st.error(f"Index '{PINECONE_INDEX_NAME}' kh√¥ng t·ªìn t·∫°i. Vui l√≤ng ch·∫°y 'preprocess_data_optimized.py' tr∆∞·ªõc.")
        st.stop()
        
    pinecone_index = pc.Index(PINECONE_INDEX_NAME)
    embedding_model = SentenceTransformer(MODEL_NAME)
    return FinancialQASystem(gemini_model, pinecone_index, embedding_model)

qa_system = initialize_services()

# --- KH·ªûI T·∫†O SESSION STATE ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "selected_ticker" not in st.session_state:
    st.session_state.selected_ticker = "NVDA"

# --- GIAO DI·ªÜN STREAMLIT ---
st.title("üìà Pro Financial Q&A with RAG")
st.markdown("H·ªá th·ªëng c√≥ th·ªÉ t·ª± ƒë·ªông ph√°t hi·ªán v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi so s√°nh gi·ªØa nhi·ªÅu c√¥ng ty.")

with st.sidebar:
    st.header("C√†i ƒë·∫∑t Truy v·∫•n")
    st.session_state.selected_ticker = st.selectbox(
        "Ch·ªçn Ticker m·∫∑c ƒë·ªãnh:", 
        options=AVAILABLE_TICKERS,
        key="ticker_selector"
    )
    st.info(f"Ticker m·∫∑c ƒë·ªãnh l√† **{st.session_state.selected_ticker}**.")
    if st.button("B·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi"):
        st.session_state.messages = []
        st.rerun()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if "contexts" in message and message["contexts"]:
             with st.expander("Xem c√°c ngu·ªìn ƒë√£ s·ª≠ d·ª•ng"):
                for i, context in enumerate(message["contexts"], 1):
                    ticker_badge = f"`{context['ticker']}`"
                    st.info(f'**Ngu·ªìn {i} ({ticker_badge}):** "{context["text"]}"')

# --- LOGIC CH√çNH ---
if prompt := st.chat_input("H·ªèi v·ªÅ m·ªôt ho·∫∑c nhi·ªÅu c√¥ng ty..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        prompt_upper = prompt.upper()
        tickers_in_prompt = sorted(list(set([
            ticker for ticker in AVAILABLE_TICKERS 
            if re.search(r'\b' + re.escape(ticker) + r'\b', prompt_upper)
        ])))

        if not tickers_in_prompt:
            tickers_to_query = [st.session_state.selected_ticker]
            st.info(f"üîç Kh√¥ng t√¨m th·∫•y ticker trong c√¢u h·ªèi. S·ª≠ d·ª•ng ticker m·∫∑c ƒë·ªãnh: **{tickers_to_query[0]}**")
        else:
            tickers_to_query = tickers_in_prompt
            st.info(f"üîç ƒê√£ ph√°t hi·ªán v√† s·∫Ω ph√¢n t√≠ch c√°c ticker: **{', '.join(tickers_to_query)}**")

        all_contexts = []
        with st.spinner(f"ƒêang t√¨m ki·∫øm th√¥ng tin cho {', '.join(tickers_to_query)}..."):
            for ticker in tickers_to_query:
                contexts = qa_system.find_top_contexts(
                    ticker=ticker,
                    question=prompt,
                    top_k=5 
                )
                all_contexts.extend(contexts)
        
        full_response = ""
        response_generator = qa_system.get_answer_stream(prompt, all_contexts)
        
        for chunk in response_generator:
            full_response += chunk
            message_placeholder.markdown(full_response + "‚ñå")
        
        message_placeholder.markdown(full_response)
        
        st.session_state.messages.append({
            "role": "assistant", 
            "content": full_response,
            "contexts": all_contexts
        })